{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMOTION RECOGNITION THROUGH VOICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import wave\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all emotions on RAVDESS dataset\n",
    "int2emotion = {\n",
    "    \"01\": \"neutral\",\n",
    "    \"02\": \"calm\",\n",
    "    \"03\": \"happy\",\n",
    "    \"04\": \"sad\",\n",
    "    \"05\": \"angry\",\n",
    "    \"06\": \"fearful\",\n",
    "    \"07\": \"disgust\",\n",
    "    \"08\": \"surprised\"\n",
    "}\n",
    "\n",
    "# we allow only these emotions ( feel free to tune this on your need )\n",
    "AVAILABLE_EMOTIONS = {\n",
    "    \"angry\",\n",
    "    \"sad\",\n",
    "    \"neutral\",\n",
    "    \"happy\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc(file_name):\n",
    "    y,sr=librosa.load(file_name)\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=y,sr=sr,n_mfcc=40),axis=1) #y represents the amplitudes .\n",
    "    #sr(sample rate) represents no.of waves per second.n_mfcc(number of MFCCs to return)is set to 40 because there can be differnt amplitudes so it will take till 40\n",
    "    #axis=1 means mean is calculated column wise(if axis=0 row wise)\n",
    "    #y,sr,n_mfcc are the inputs to librosa.feature.mfcc fuction which calcutes the mfcc feature once it is calculated mean of it is taken col wise\n",
    "    return(mfccs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 166)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y,sr=librosa.load(r'C:\\Users\\HP\\Desktop\\Dataset\\emotions dataset\\Actor_10\\03-01-01-01-01-02-10.wav') # entire path has to be given\n",
    "x=librosa.feature.mfcc(y=y,sr=sr)\n",
    "x.shape\n",
    "#(20,166) means mfcc is not a single value it is got 20 rows n 166 col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=np.mean(x,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.2902622e-05, -6.9945891e-05,  1.1423043e-04, ...,\n",
       "        8.7954162e-05,  8.6632572e-05,  0.0000000e+00], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22050"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sr # sr is a single value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.83718820861678"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape[0]/sr # y gives wav  sr gives wav/sec so wav/(wav/sec) gives sec i.e time...here time of the audio recorded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract audio data from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dic='C:/Users/HP/Desktop/Dataset/emotions dataset'\n",
    "data=[]\n",
    "labels=[]\n",
    "for subdirs,dirs,files in os.walk(root_dic):\n",
    "    for file in files:\n",
    "        temp_lable = int2emotion[str(file[6:8])]\n",
    "        if temp_lable in AVAILABLE_EMOTIONS:\n",
    "            y,sr=librosa.load(os.path.join(subdirs,file)) #for librosa load we have to give the entire path so we join subdir(path for dictonary) and file(.wav file)\n",
    "            mfccs = np.mean(librosa.feature.mfcc(y=y,sr=sr,n_mfcc=40),axis=1)\n",
    "            data.append(mfccs)\n",
    "            labels.append(int(file[7:8])-1) # file name 03-01-06-01-01-01-11 index value 0,1..... it takes 7th index value i.e the lable for the emotion\n",
    "        # -1 will make the labels from 0-7 rather than 1-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(672, 40)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#store them to arrays for testing\n",
    "data_array=np.array(data)\n",
    "labels_array=np.array(labels)\n",
    "data_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(672, 40)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(672,)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### load data from savee dataset\n",
    "#### although, we load the data here, it is not used in training or validation\n",
    "root_dir = \"C:/Users/HP/Desktop/Dataset/emotions dataset2(savee)/AudioData\"\n",
    "# root_dir = \"../input/audio_speech_actors_01-24/\"\n",
    "savee_data = []\n",
    "savee_labels = []\n",
    "for actor_dir in sorted(os.listdir(root_dir)):\n",
    "    if actor_dir[-4:] == \".txt\":\n",
    "        continue\n",
    "    for file_name in os.listdir(os.path.join(root_dir, actor_dir)):\n",
    "        if file_name[0] == \"c\":\n",
    "            continue\n",
    "        wav_file_name = os.path.join(root_dir, actor_dir, file_name)\n",
    "        if file_name[0] == \"n\":\n",
    "            savee_labels.append(0)\n",
    "            savee_data.append(extract_mfcc(wav_file_name))\n",
    "        if file_name[0] == \"a\":\n",
    "            savee_labels.append(4)\n",
    "            savee_data.append(extract_mfcc(wav_file_name))\n",
    "        if file_name[0] == \"h\":\n",
    "            savee_labels.append(2)\n",
    "            savee_data.append(extract_mfcc(wav_file_name))\n",
    "        if file_name[:2] == \"sa\":\n",
    "            savee_labels.append(3)\n",
    "            savee_data.append(extract_mfcc(wav_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "savee_data_array = np.asarray(savee_data)\n",
    "savee_label_array = np.array(savee_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 40)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "savee_data_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "savee_label_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load RAVDESS song data\n",
    "root_dir = \"C:/Users/HP/Desktop/Dataset/emotion dataset3(song)\"\n",
    "radvess_song_labels = []\n",
    "ravdess_song_data = []\n",
    "for actor_dir in sorted(os.listdir(root_dir)):\n",
    "    actor_name = os.path.join(root_dir, actor_dir)\n",
    "    for file in os.listdir(actor_name):\n",
    "        temp_lable = int2emotion[str(file[6:8])]\n",
    "        if temp_lable in AVAILABLE_EMOTIONS:\n",
    "            radvess_song_labels.append(int(file[7:8])-1)\n",
    "            wav_file_name = os.path.join(root_dir, actor_dir, file)\n",
    "            ravdess_song_data.append(extract_mfcc(wav_file_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(644, 40)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ravdess_song_data_array = np.asarray(ravdess_song_data)\n",
    "ravdess_song_label_array = np.array(radvess_song_labels)\n",
    "ravdess_song_data_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(644,)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ravdess_song_label_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1616,)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #### combine data\n",
    "data_f= np.r_[data_array, savee_data_array,ravdess_song_data_array]\n",
    "labels_f= np.r_[labels_array,savee_label_array,ravdess_song_label_array ]\n",
    "# data = ravdess_speech_data_array\n",
    "# labels = ravdess_speech_label_array\n",
    "labels_f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 4, 4, 4])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAOGElEQVR4nO3dW4xd1X3H8e8/toFItDjgSWrZbocqVhVaNUAs6gqpQpAHLhFGqlEdtWCQI0stUYmolDp5KGrVB/ISItoqkRujmjQNIBIFlxBVlItQH6AdLiFQN8VBNIxA8YSLSUSTyMm/D2eZDOMznj0z5zL+9/uRjs7ea62Z/Z/l2b+zZ52LIzORJNXyrnEXIEkaPMNdkgoy3CWpIMNdkgoy3CWpoNXjLgBg3bp1OTk5Oe4yJOmk8sQTT/wgMyf69a2IcJ+cnGRqamrcZUjSSSUi/me+PpdlJKkgw12SCjLcJakgw12SCjLcJakgw12SCjLcJakgw12SCjLcJamgFfEOVWmlmtzzjbEd+8VbrhjLcf2ZR2tYP7NX7pJUkOEuSQUZ7pJUkOEuSQUZ7pJUkOEuSQUZ7pJUkOEuSQUZ7pJUkOEuSQUZ7pJUkOEuSQUZ7pJUUOdwj4hVEfFURNzX9s+OiMcj4vmIuCsiTmntp7b9Q61/cjilS5Lms5gr9xuBg7P2PwPcmpmbgdeBXa19F/B6Zr4fuLWNkySNUKdwj4iNwBXAF9t+ABcD97Qh+4Gr2va2tk/rv6SNlySNSNcr988BnwR+3vbPAt7IzKNtfxrY0LY3AC8BtP4jbfw7RMTuiJiKiKmZmZklli9J6mfBcI+IjwCHM/OJ2c19hmaHvl80ZO7NzC2ZuWViYqJTsZKkbrr8N3sXAldGxOXAacAv07uSXxsRq9vV+Ubg5TZ+GtgETEfEauAM4LWBVy5JmteCV+6Z+anM3JiZk8AO4KHM/EPgYWB7G7YTuLdtH2j7tP6HMvO4K3dJ0vAs53Xufw7cFBGH6K2p72vt+4CzWvtNwJ7llShJWqwuyzJvy8xHgEfa9gvABX3G/Bi4egC1SZKWyHeoSlJBhrskFWS4S1JBhrskFWS4S1JBhrskFWS4S1JBhrskFWS4S1JBhrskFWS4S1JBhrskFWS4S1JBhrskFWS4S1JBhrskFWS4S1JBhrskFWS4S1JBhrskFWS4S1JBhrskFWS4S1JBhrskFWS4S1JBhrskFWS4S1JBhrskFWS4S1JBhrskFWS4S1JBhrskFWS4S1JBhrskFWS4S1JBhrskFWS4S1JBhrskFbR6oQERcRrwKHBqG39PZt4cEWcDdwJnAk8C12TmTyPiVOAO4EPAq8AfZOaLQ6qfyT3fGNa3XtCLt1wxtmNL0ol0uXL/CXBxZn4QOBe4NCK2Ap8Bbs3MzcDrwK42fhfwema+H7i1jZMkjdCC4Z49P2q7a9otgYuBe1r7fuCqtr2t7dP6L4mIGFjFkqQFdVpzj4hVEfE0cBh4APgu8EZmHm1DpoENbXsD8BJA6z8CnNXne+6OiKmImJqZmVneTyFJeodO4Z6ZP8vMc4GNwAXAB/oNa/f9rtLzuIbMvZm5JTO3TExMdK1XktTBol4tk5lvAI8AW4G1EXHsCdmNwMttexrYBND6zwBeG0SxkqRuFgz3iJiIiLVt+93Ah4GDwMPA9jZsJ3Bv2z7Q9mn9D2XmcVfukqThWfClkMB6YH9ErKL3YHB3Zt4XEf8J3BkRfw08Bexr4/cBX4qIQ/Su2HcMoW5J0gksGO6Z+QxwXp/2F+itv89t/zFw9UCqkyQtie9QlaSCDHdJKshwl6SCDHdJKshwl6SCDHdJKshwl6SCDHdJKshwl6SCDHdJKshwl6SCDHdJKshwl6SCDHdJKshwl6SCDHdJKshwl6SCDHdJKshwl6SCDHdJKshwl6SCDHdJKshwl6SCDHdJKshwl6SCDHdJKshwl6SCDHdJKshwl6SCDHdJKshwl6SCDHdJKshwl6SCDHdJKshwl6SCDHdJKshwl6SCDHdJKshwl6SCFgz3iNgUEQ9HxMGIeC4ibmztZ0bEAxHxfLt/T2uPiLgtIg5FxDMRcf6wfwhJ0jt1uXI/CvxZZn4A2ArcEBHnAHuABzNzM/Bg2we4DNjcbruBzw+8aknSCS0Y7pn5SmY+2bZ/CBwENgDbgP1t2H7gqra9Dbgjex4D1kbE+oFXLkma16LW3CNiEjgPeBx4X2a+Ar0HAOC9bdgG4KVZXzbd2uZ+r90RMRURUzMzM4uvXJI0r87hHhGnA18FPpGZb55oaJ+2PK4hc29mbsnMLRMTE13LkCR10CncI2INvWD/cmZ+rTV//9hyS7s/3NqngU2zvnwj8PJgypUkddHl1TIB7AMOZuZnZ3UdAHa27Z3AvbPar22vmtkKHDm2fCNJGo3VHcZcCFwDfDsinm5tnwZuAe6OiF3A94CrW9/9wOXAIeAt4PqBVixJWtCC4Z6Z/0b/dXSAS/qMT+CGZdYlSVoG36EqSQUZ7pJUkOEuSQUZ7pJUkOEuSQUZ7pJUkOEuSQUZ7pJUkOEuSQUZ7pJUkOEuSQUZ7pJUkOEuSQUZ7pJUkOEuSQUZ7pJUkOEuSQUZ7pJUkOEuSQUZ7pJUkOEuSQUZ7pJUkOEuSQUZ7pJUkOEuSQUZ7pJUkOEuSQUZ7pJUkOEuSQUZ7pJUkOEuSQUZ7pJUkOEuSQUZ7pJUkOEuSQUZ7pJUkOEuSQUZ7pJU0ILhHhG3R8ThiHh2VtuZEfFARDzf7t/T2iMibouIQxHxTEScP8ziJUn9dbly/wfg0jlte4AHM3Mz8GDbB7gM2Nxuu4HPD6ZMSdJiLBjumfko8Nqc5m3A/ra9H7hqVvsd2fMYsDYi1g+qWElSN0tdc39fZr4C0O7f29o3AC/NGjfd2o4TEbsjYioipmZmZpZYhiSpn0E/oRp92rLfwMzcm5lbMnPLxMTEgMuQpP/flhru3z+23NLuD7f2aWDTrHEbgZeXXp4kaSmWGu4HgJ1teydw76z2a9urZrYCR44t30iSRmf1QgMi4ivARcC6iJgGbgZuAe6OiF3A94Cr2/D7gcuBQ8BbwPVDqFmStIAFwz0zPzpP1yV9xiZww3KLkiQtj+9QlaSCDHdJKshwl6SCDHdJKshwl6SCDHdJKshwl6SCDHdJKshwl6SCDHdJKshwl6SCDHdJKshwl6SCDHdJKshwl6SCDHdJKshwl6SCDHdJKshwl6SCDHdJKshwl6SCDHdJKshwl6SCDHdJKshwl6SCDHdJKshwl6SCDHdJKshwl6SCDHdJKshwl6SCDHdJKshwl6SCDHdJKshwl6SCDHdJKshwl6SCDHdJKshwl6SChhLuEXFpRHwnIg5FxJ5hHEOSNL+Bh3tErAL+DrgMOAf4aEScM+jjSJLmN4wr9wuAQ5n5Qmb+FLgT2DaE40iS5hGZOdhvGLEduDQzP9b2rwF+JzM/PmfcbmB32/0N4DtLPOQ64AdL/Nphsq7Fsa7FW6m1WdfiLKeuX8vMiX4dq5dez7yiT9txjyCZuRfYu+yDRUxl5pblfp9Bs67Fsa7FW6m1WdfiDKuuYSzLTAObZu1vBF4ewnEkSfMYRrj/B7A5Is6OiFOAHcCBIRxHkjSPgS/LZObRiPg48C/AKuD2zHxu0MeZZdlLO0NiXYtjXYu3UmuzrsUZSl0Df0JVkjR+vkNVkgoy3CWpoJMm3Bf6SIOIODUi7mr9j0fE5Aqp67qImImIp9vtYyOq6/aIOBwRz87THxFxW6v7mYg4f4XUdVFEHJk1X38xgpo2RcTDEXEwIp6LiBv7jBn5fHWsaxzzdVpE/HtEfKvV9Zd9xoz8fOxY11jOx3bsVRHxVETc16dv8POVmSv+Ru+J2e8Cvw6cAnwLOGfOmD8BvtC2dwB3rZC6rgP+dgxz9nvA+cCz8/RfDnyT3vsStgKPr5C6LgLuG/FcrQfOb9u/BPx3n3/Hkc9Xx7rGMV8BnN621wCPA1vnjBnH+dilrrGcj+3YNwH/1O/faxjzdbJcuXf5SINtwP62fQ9wSUT0e0PVqOsai8x8FHjtBEO2AXdkz2PA2ohYvwLqGrnMfCUzn2zbPwQOAhvmDBv5fHWsa+TaHPyo7a5pt7mvzBj5+dixrrGIiI3AFcAX5xky8Pk6WcJ9A/DSrP1pjv8lf3tMZh4FjgBnrYC6AH6//Sl/T0Rs6tM/Dl1rH4ffbX9afzMifnOUB25/Dp9H76pvtrHO1wnqgjHMV1tieBo4DDyQmfPO1wjPxy51wXjOx88BnwR+Pk//wOfrZAn3Lh9p0OljDwasyzH/GZjMzN8G/pVfPDqP2zjmq4sn6X1exgeBvwG+PqoDR8TpwFeBT2Tmm3O7+3zJSOZrgbrGMl+Z+bPMPJfeO9AviIjfmjNkLPPVoa6Rn48R8RHgcGY+caJhfdqWNV8nS7h3+UiDt8dExGrgDIb/5/+CdWXmq5n5k7b798CHhlxTVyvyYyIy881jf1pn5v3AmohYN+zjRsQaegH65cz8Wp8hY5mvheoa13zNOv4bwCPApXO6xnE+LljXmM7HC4ErI+JFeku3F0fEP84ZM/D5OlnCvctHGhwAdrbt7cBD2Z6dGGddc9Zlr6S3broSHACuba8C2QocycxXxl1URPzKsbXGiLiA3u/oq0M+ZgD7gIOZ+dl5ho18vrrUNab5moiItW373cCHgf+aM2zk52OXusZxPmbmpzJzY2ZO0suIhzLzj+YMG/h8DeNTIQcu5/lIg4j4K2AqMw/QOwm+FBGH6D3i7Vghdf1pRFwJHG11XTfsugAi4iv0XkmxLiKmgZvpPcFEZn4BuJ/eK0AOAW8B16+QurYDfxwRR4H/BXaM4EH6QuAa4NttvRbg08CvzqprHPPVpa5xzNd6YH/0/mOedwF3Z+Z94z4fO9Y1lvOxn2HPlx8/IEkFnSzLMpKkRTDcJakgw12SCjLcJakgw12SCjLcJakgw12SCvo/pU7n7dYp92kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(labels_f)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1616, 5)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#changing labels to categorical\n",
    "labels_categorical=to_categorical(labels_f)\n",
    "data_f.shape\n",
    "labels_categorical.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "672"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.optimizers import rmsprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_CNN():\n",
    "    ### CNN model, referred to the model B in the report\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(8, kernel_size = 3, input_shape=(40, 1)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Conv1D(16,kernel_size = 3))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv1D(32, kernel_size = 3))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Conv1D(16, kernel_size = 3))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization(axis=-1))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(5))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(data_f,labels_categorical,random_state=7,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number_of_samples = data_f.shape[0]\n",
    "#training_samples = int(number_of_samples * 0.8)\n",
    "#validation_samples = int(number_of_samples * 0.1)\n",
    "#test_samples = int(number_of_samples * 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B = create_model_CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1131 samples, validate on 485 samples\n",
      "Epoch 1/20\n",
      "1131/1131 [==============================] - 1s 1ms/step - loss: 0.5393 - acc: 0.7683 - val_loss: 0.5696 - val_acc: 0.7732\n",
      "Epoch 2/20\n",
      "1131/1131 [==============================] - 1s 1ms/step - loss: 0.5212 - acc: 0.7931 - val_loss: 0.5563 - val_acc: 0.7814\n",
      "Epoch 3/20\n",
      "1131/1131 [==============================] - 1s 1ms/step - loss: 0.4966 - acc: 0.8073 - val_loss: 0.5502 - val_acc: 0.7794\n",
      "Epoch 4/20\n",
      "1131/1131 [==============================] - 1s 1ms/step - loss: 0.5112 - acc: 0.7949 - val_loss: 0.5346 - val_acc: 0.7979\n",
      "Epoch 5/20\n",
      "1131/1131 [==============================] - 1s 1ms/step - loss: 0.5408 - acc: 0.7966 - val_loss: 0.5476 - val_acc: 0.7835\n",
      "Epoch 6/20\n",
      "1131/1131 [==============================] - 1s 1ms/step - loss: 0.5076 - acc: 0.8002 - val_loss: 0.5640 - val_acc: 0.7691\n",
      "Epoch 7/20\n",
      "1131/1131 [==============================] - 1s 1ms/step - loss: 0.5172 - acc: 0.7940 - val_loss: 0.5565 - val_acc: 0.7856\n",
      "Epoch 8/20\n",
      "1131/1131 [==============================] - 1s 1ms/step - loss: 0.4902 - acc: 0.7975 - val_loss: 0.5640 - val_acc: 0.7856\n",
      "Epoch 9/20\n",
      "1131/1131 [==============================] - 1s 1ms/step - loss: 0.5039 - acc: 0.7993 - val_loss: 0.5882 - val_acc: 0.7691\n",
      "Epoch 10/20\n",
      "1131/1131 [==============================] - 2s 1ms/step - loss: 0.5121 - acc: 0.8055 - val_loss: 0.5663 - val_acc: 0.7794\n",
      "Epoch 11/20\n",
      "1131/1131 [==============================] - 1s 1ms/step - loss: 0.5121 - acc: 0.8108 - val_loss: 0.5771 - val_acc: 0.7856\n",
      "Epoch 12/20\n",
      "1131/1131 [==============================] - 2s 1ms/step - loss: 0.5076 - acc: 0.7905 - val_loss: 0.5456 - val_acc: 0.7897\n",
      "Epoch 13/20\n",
      "1131/1131 [==============================] - 2s 1ms/step - loss: 0.4984 - acc: 0.8064 - val_loss: 0.5612 - val_acc: 0.7814\n",
      "Epoch 14/20\n",
      "1131/1131 [==============================] - 2s 1ms/step - loss: 0.4893 - acc: 0.7984 - val_loss: 0.5551 - val_acc: 0.7856\n",
      "Epoch 15/20\n",
      "1131/1131 [==============================] - 2s 1ms/step - loss: 0.5066 - acc: 0.8002 - val_loss: 0.5461 - val_acc: 0.7856\n",
      "Epoch 16/20\n",
      "1131/1131 [==============================] - 2s 2ms/step - loss: 0.4497 - acc: 0.8311 - val_loss: 0.5488 - val_acc: 0.7794\n",
      "Epoch 17/20\n",
      "1131/1131 [==============================] - 1s 1ms/step - loss: 0.4859 - acc: 0.8152 - val_loss: 0.5454 - val_acc: 0.7938\n",
      "Epoch 18/20\n",
      "1131/1131 [==============================] - 2s 1ms/step - loss: 0.4704 - acc: 0.8046 - val_loss: 0.5545 - val_acc: 0.7773\n",
      "Epoch 19/20\n",
      "1131/1131 [==============================] - 2s 1ms/step - loss: 0.5006 - acc: 0.7940 - val_loss: 0.5523 - val_acc: 0.7876\n",
      "Epoch 20/20\n",
      "1131/1131 [==============================] - 2s 2ms/step - loss: 0.4824 - acc: 0.8179 - val_loss: 0.5496 - val_acc: 0.7938\n"
     ]
    }
   ],
   "source": [
    "history = model_B.fit(np.expand_dims(X_train,-1), Y_train, \n",
    "                      validation_data=(np.expand_dims(X_test ,-1), \n",
    "                                       Y_test), epochs=100,\n",
    "                      shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  evaluate  model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "485/485 [==============================] - 1s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5496478827958254, 0.7938144332354831]"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B.evaluate(np.expand_dims(X_test, -1), Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B.save('4EmotionsModel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please talk\n",
      "happy\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import os\n",
    "import wave\n",
    "import pickle\n",
    "from sys import byteorder\n",
    "from array import array\n",
    "from struct import pack\n",
    "\n",
    "intemotion = {\n",
    "    0: \"neutral\",\n",
    "    1: \"calm\",\n",
    "    2: \"happy\",\n",
    "    3: \"sad\",\n",
    "    4: \"angry\",\n",
    "    5: \"fearful\",\n",
    "    6: \"disgust\",\n",
    "    7: \"surprised\"\n",
    "}\n",
    "\n",
    "\n",
    "THRESHOLD = 500\n",
    "CHUNK_SIZE = 1024\n",
    "FORMAT = pyaudio.paInt16\n",
    "RATE = 16000\n",
    "\n",
    "SILENCE = 30\n",
    "\n",
    "def is_silent(snd_data):\n",
    "    \"Returns 'True' if below the 'silent' threshold\"\n",
    "    return max(snd_data) < THRESHOLD\n",
    "\n",
    "def normalize(snd_data):\n",
    "    \"Average the volume out\"\n",
    "    MAXIMUM = 16384\n",
    "    times = float(MAXIMUM)/max(abs(i) for i in snd_data)\n",
    "\n",
    "    r = array('h')\n",
    "    for i in snd_data:\n",
    "        r.append(int(i*times))\n",
    "    return r\n",
    "\n",
    "def trim(snd_data):\n",
    "    \"Trim the blank spots at the start and end\"\n",
    "    def _trim(snd_data):\n",
    "        snd_started = False\n",
    "        r = array('h')\n",
    "\n",
    "        for i in snd_data:\n",
    "            if not snd_started and abs(i)>THRESHOLD:\n",
    "                snd_started = True\n",
    "                r.append(i)\n",
    "\n",
    "            elif snd_started:\n",
    "                r.append(i)\n",
    "        return r\n",
    "\n",
    "    # Trim to the left\n",
    "    snd_data = _trim(snd_data)\n",
    "\n",
    "    # Trim to the right\n",
    "    snd_data.reverse()\n",
    "    snd_data = _trim(snd_data)\n",
    "    snd_data.reverse()\n",
    "    return snd_data\n",
    "\n",
    "def add_silence(snd_data, seconds):\n",
    "    \"Add silence to the start and end of 'snd_data' of length 'seconds' (float)\"\n",
    "    r = array('h', [0 for i in range(int(seconds*RATE))])\n",
    "    r.extend(snd_data)\n",
    "    r.extend([0 for i in range(int(seconds*RATE))])\n",
    "    return r\n",
    "\n",
    "def record():\n",
    "    \"\"\"\n",
    "    Record a word or words from the microphone and \n",
    "    return the data as an array of signed shorts.\n",
    "    Normalizes the audio, trims silence from the \n",
    "    start and end, and pads with 0.5 seconds of \n",
    "    blank sound to make sure VLC et al can play \n",
    "    it without getting chopped off.\n",
    "    \"\"\"\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(format=FORMAT, channels=1, rate=RATE,\n",
    "        input=True, output=True,\n",
    "        frames_per_buffer=CHUNK_SIZE)\n",
    "\n",
    "    num_silent = 0\n",
    "    snd_started = False\n",
    "\n",
    "    r = array('h')\n",
    "\n",
    "    while 1:\n",
    "        # little endian, signed short\n",
    "        snd_data = array('h', stream.read(CHUNK_SIZE))\n",
    "        if byteorder == 'big':\n",
    "            snd_data.byteswap()\n",
    "        r.extend(snd_data)\n",
    "\n",
    "        silent = is_silent(snd_data)\n",
    "\n",
    "        if silent and snd_started:\n",
    "            num_silent += 1\n",
    "        elif not silent and not snd_started:\n",
    "            snd_started = True\n",
    "\n",
    "        if snd_started and num_silent > SILENCE:\n",
    "            break\n",
    "\n",
    "    sample_width = p.get_sample_size(FORMAT)\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "\n",
    "    r = normalize(r)\n",
    "    r = trim(r)\n",
    "    r = add_silence(r, 0.5)\n",
    "    return sample_width, r\n",
    "\n",
    "def record_to_file(path):\n",
    "    \"Records from the microphone and outputs the resulting data to 'path'\"\n",
    "    sample_width, data = record()\n",
    "    data = pack('<' + ('h'*len(data)), *data)\n",
    "\n",
    "    wf = wave.open(path, 'wb')\n",
    "    wf.setnchannels(1)\n",
    "    wf.setsampwidth(sample_width)\n",
    "    wf.setframerate(RATE)\n",
    "    wf.writeframes(data)\n",
    "    wf.close()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # load the saved model (after training)\n",
    "#     model = pickle.load(open(\"result/mlp_classifier.model\", \"rb\"))\n",
    "    print(\"Please talk\")\n",
    "    filename = \"test.wav\"\n",
    "    # record the file (start talking)\n",
    "    record_to_file(filename)\n",
    "    # extract features and reshape it\n",
    "    features = extract_mfcc(filename)\n",
    "    # predict\n",
    "    result = model_B.predict(features.reshape(1,40,1))[0]\n",
    "    # show the result !\n",
    "#     print(\"result:\", result)\n",
    "    print(intemotion[np.argmax(result)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
